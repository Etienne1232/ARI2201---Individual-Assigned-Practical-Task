{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Triples2Text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the Benchmark object that was supplied with the data to parse the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:23.794150200Z",
     "start_time": "2023-08-20T01:42:20.643038400Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.benchmark_reader import Benchmark\n",
    "from utils.benchmark_reader import select_files\n",
    "\n",
    "b = Benchmark()\n",
    "files = select_files(\"./data/mt_train.xml\")\n",
    "b.fill_benchmark(files)\n",
    "initial_data_dict = b.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Familiarised myself with the dict. \n",
    "<li> Flattened the first dimension of the dict, as it had only 1 element, so that dimension was redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:23.805094Z",
     "start_time": "2023-08-20T01:42:23.795147700Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_data_dict = initial_data_dict['entries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Found that the dict had length 13211."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:23.894712Z",
     "start_time": "2023-08-20T01:42:23.816581500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "13211"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dict was in the form of a list, where each element in the list held a dict with 1 element. This could also be flattened. The dicts started from index 1, so that was fixed and reverted to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:23.895705400Z",
     "start_time": "2023-08-20T01:42:23.833427500Z"
    }
   },
   "outputs": [],
   "source": [
    "flattened_dict = {}\n",
    "\n",
    "for dictionary in initial_data_dict:\n",
    "    key, value = list(dictionary.items())[0]\n",
    "\n",
    "    flattened_dict[key-1] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From reading the documentation, I understood that I should be using the 'modifiedtripleset' element as a triple, as this is the most readable and accurate. <br> <br>\n",
    "'lexicalisations' is a list of dictionaries with correct results. These results are in both English and Maltese depending on the 'lang' attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:55.011584600Z",
     "start_time": "2023-08-20T01:42:23.883948600Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change sm to lg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:56.224969400Z",
     "start_time": "2023-08-20T01:42:55.015030500Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 9999999"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenization, Lemmatization and Excluding Punctuation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "first10 = {}\n",
    "for x in range(10):\n",
    "    first10.update({x: flattened_dict[x]})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:56.236517700Z",
     "start_time": "2023-08-20T01:42:56.227959700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "getting and tokenizing modifiedtripleset & lexicalisations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def tokenize_lemmatize_punct(triple_value):\n",
    "    return [token.lemma_ for token in nlp(triple_value) if token.text not in string.punctuation]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T21:03:41.568662800Z",
     "start_time": "2023-08-19T21:03:41.545641100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-19T22:12:25.695109100Z",
     "start_time": "2023-08-19T22:12:25.673439900Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_and_punct_of_whole_dict(flattened_dict):\n",
    "    result = []\n",
    "    for idx in range(len(flattened_dict)):\n",
    "        # initialising lists\n",
    "        all_triples_per_dict_entry = []\n",
    "        processed_item = {}\n",
    "        lexes = []\n",
    "\n",
    "        #iterating through list of triples and tokenising\n",
    "        for triple_set in flattened_dict[idx]['modifiedtripleset']:\n",
    "            processed_item = {\n",
    "            'subject': tokenize_lemmatize_punct(triple_set['subject']),\n",
    "            'property': tokenize_lemmatize_punct(triple_set['property']),\n",
    "            'object': tokenize_lemmatize_punct(triple_set['object'])\n",
    "            }\n",
    "            all_triples_per_dict_entry.append(processed_item)\n",
    "\n",
    "        #iterating through list of lexicalisations\n",
    "        for lexicalisation in flattened_dict[idx]['lexicalisations']:\n",
    "            #if language is\n",
    "            if lexicalisation['lang'] == '':\n",
    "                #the cleaned lexicalisation is appended to a list\n",
    "                lexes.append(tokenize_lemmatize_punct(lexicalisation ['lex']))\n",
    "\n",
    "        result.append([all_triples_per_dict_entry, lexes])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "token_punct = token_and_punct_of_whole_dict(flattened_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T22:29:16.407197800Z",
     "start_time": "2023-08-19T22:12:29.023713900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:56.271812500Z",
     "start_time": "2023-08-20T01:42:56.238512700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "json_object = json.dumps(token_punct)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:38:33.999892500Z",
     "start_time": "2023-08-20T01:38:33.878294300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "with open('sample.json', 'r') as openfile:\n",
    "\n",
    "    # Reading from json file\n",
    "    token_punct = json.load(openfile)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:42:57.985805200Z",
     "start_time": "2023-08-20T01:42:56.251550400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#returns list of list of dicts\n",
    "def get_triples_from_dict(flattened_dict):\n",
    "    result = []\n",
    "    for idx in range(len(flattened_dict)):\n",
    "        result.append(flattened_dict[idx]['modifiedtripleset'])\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T20:56:55.985460100Z",
     "start_time": "2023-08-19T20:56:55.968958100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "triple_lists = get_triples_from_dict(flattened_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T20:56:56.013571600Z",
     "start_time": "2023-08-19T20:56:55.982468300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T01:43:56.146867200Z",
     "start_time": "2023-08-20T01:43:56.131913Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(cleaned_data):\n",
    "    vocab = set()\n",
    "\n",
    "    for entry in cleaned_data:\n",
    "        triple = entry[0]\n",
    "        lex = entry[1]\n",
    "\n",
    "        for current_triple in triple:\n",
    "            for token in current_triple['object']:\n",
    "                vocab.add(token)\n",
    "            for token in current_triple['property']:\n",
    "                vocab.add(token)\n",
    "            for token in current_triple['subject']:\n",
    "                vocab.add(token)\n",
    "\n",
    "        #add every token in lexicalisation\n",
    "        for current_lex in lex:\n",
    "            for token in current_lex:\n",
    "                vocab.add(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "vocab_list = get_vocab(token_punct)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:43:56.506533300Z",
     "start_time": "2023-08-20T01:43:56.336284Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One-Hot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:51:44.257105500Z",
     "start_time": "2023-08-20T01:51:42.839557400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:43:30.332043700Z",
     "start_time": "2023-08-20T01:43:30.320226Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "data = {'word':list(vocab_list)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:55:40.087231100Z",
     "start_time": "2023-08-20T01:55:40.061224100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:55:40.695044Z",
     "start_time": "2023-08-20T01:55:40.640727500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "one_hot_array = encoder.fit_transform(df[['word']]).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:55:58.269574600Z",
     "start_time": "2023-08-20T01:55:58.056406900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "one_hot_df = pd.DataFrame(one_hot_array, columns=encoder.get_feature_names_out())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T01:59:23.757788500Z",
     "start_time": "2023-08-20T01:59:23.677051300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T17:08:54.205365Z",
     "start_time": "2023-08-20T17:08:54.167919400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def model_creation(vocabulary_size, sequence_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, sequence_len, input_length=sequence_len))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T18:22:58.992313700Z",
     "start_time": "2023-08-20T18:22:58.943101400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_creation(vocab_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
